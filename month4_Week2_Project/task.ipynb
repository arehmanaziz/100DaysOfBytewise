{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing a Basic RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 134ms/step - accuracy: 0.5785 - loss: 0.6656 - val_accuracy: 0.7794 - val_loss: 0.4800\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 127ms/step - accuracy: 0.8339 - loss: 0.3813 - val_accuracy: 0.8334 - val_loss: 0.3859\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 112ms/step - accuracy: 0.9186 - loss: 0.2170 - val_accuracy: 0.8400 - val_loss: 0.3750\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.9718 - loss: 0.0996 - val_accuracy: 0.8248 - val_loss: 0.4397\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 102ms/step - accuracy: 0.9893 - loss: 0.0478 - val_accuracy: 0.8372 - val_loss: 0.4900\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.8305 - loss: 0.5202\n",
      "Test Loss: 0.5169132351875305, Test Accuracy: 0.8324800133705139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "\n",
    "# Loading the IMDB dataset\n",
    "max_features = 10000  # Only consider the top 10,000 words\n",
    "max_len = 500  # Only consider the first 500 words of each review\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# Building the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  # Embedding layer\n",
    "model.add(SimpleRNN(32))  # RNN layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "* We are using the IMDB dataset for sentiment analysis.\n",
    "* The Embedding layer converts the words into vectors.\n",
    "* The SimpleRNN layer processes the sequence of word embeddings.\n",
    "* The Dense layer with a sigmoid activation function outputs the probability of the review being positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stacking RNN Layers and Bi-directional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (A) Stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 195ms/step - accuracy: 0.6261 - loss: 0.6152 - val_accuracy: 0.8076 - val_loss: 0.4286\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 192ms/step - accuracy: 0.8463 - loss: 0.3731 - val_accuracy: 0.8644 - val_loss: 0.3405\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 253ms/step - accuracy: 0.9062 - loss: 0.2403 - val_accuracy: 0.8292 - val_loss: 0.4081\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 224ms/step - accuracy: 0.9550 - loss: 0.1345 - val_accuracy: 0.8062 - val_loss: 0.5056\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - accuracy: 0.9820 - loss: 0.0617 - val_accuracy: 0.8192 - val_loss: 0.6122\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.8135 - loss: 0.6572\n",
      "Test Loss: 0.6360751390457153, Test Accuracy: 0.8187599778175354\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "# Building a stacked RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  # Embedding layer\n",
    "model.add(SimpleRNN(32, return_sequences=True))  # First RNN layer\n",
    "model.add(SimpleRNN(32))  # Second RNN layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (B) Bi-Directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 120ms/step - accuracy: 0.5537 - loss: 0.6799 - val_accuracy: 0.7282 - val_loss: 0.5614\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 126ms/step - accuracy: 0.8022 - loss: 0.4568 - val_accuracy: 0.8074 - val_loss: 0.4322\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 127ms/step - accuracy: 0.8877 - loss: 0.2881 - val_accuracy: 0.8204 - val_loss: 0.4224\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 141ms/step - accuracy: 0.9476 - loss: 0.1534 - val_accuracy: 0.8248 - val_loss: 0.4542\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 123ms/step - accuracy: 0.9814 - loss: 0.0722 - val_accuracy: 0.8154 - val_loss: 0.5442\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.8088 - loss: 0.5571\n",
      "Test Loss: 0.5470931529998779, Test Accuracy: 0.8123999834060669\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Building a bi-directional RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  # Embedding layer\n",
    "model.add(Bidirectional(SimpleRNN(32)))  # Bi-Directional RNN layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "* Stacked RNN: This model has two RNN layers stacked on top of each other, which helps in capturing more complex patterns in the sequences.\n",
    "* Bi-Directional RNN: This model uses a bi-directional RNN to process the sequence from both directions (past and future), which helps in better understanding the context of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring Hybrid Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - accuracy: 0.5597 - loss: 0.6676 - val_accuracy: 0.8142 - val_loss: 0.4251\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.8528 - loss: 0.3504 - val_accuracy: 0.8578 - val_loss: 0.3480\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.9116 - loss: 0.2291 - val_accuracy: 0.8426 - val_loss: 0.3585\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - accuracy: 0.9457 - loss: 0.1563 - val_accuracy: 0.8668 - val_loss: 0.3991\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.9658 - loss: 0.1014 - val_accuracy: 0.8742 - val_loss: 0.3800\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.8566 - loss: 0.4122\n",
      "Test Loss: 0.41303208470344543, Test Accuracy: 0.8555200099945068\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# Building a hybrid model combining CNN + RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  # Embedding layer\n",
    "\n",
    "# CNN layers to extract features from sequences\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# RNN layer to model temporal dependencies\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "* The CNN layer extracts local features from the sequences.\n",
    "* The MaxPooling1D layer reduces the dimensionality and focuses on the most relevant features.\n",
    "* The RNN layer processes the sequence over time.\n",
    "* This hybrid architecture can capture both local and temporal patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Basic RNN: The basic RNN model performed well on the IMDB dataset but had limitations in capturing long-term dependencies. This is because RNNs struggle with vanishing gradients for long sequences.\n",
    "\n",
    "* Stacked RNN: The stacked RNN provided a slight improvement in performance due to its ability to capture more complex patterns. However, adding more layers also increased training time.\n",
    "\n",
    "* Bi-Directional RNN: The bi-directional RNN showed a noticeable performance boost, as it processed the sequences in both directions (past and future), leading to better context understanding.\n",
    "\n",
    "* Hybrid CNN + RNN: This model outperformed the previous ones by combining the feature extraction power of CNNs with the sequential modeling capabilities of RNNs. However, it required careful tuning of hyperparameters like the kernel size and pooling window for optimal performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
